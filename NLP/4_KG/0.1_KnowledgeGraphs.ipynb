{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b9c1dde0b2464ff6778d75d84efd3ba2ef21848"
   },
   "source": [
    "## [spaCy : Faster Natural Language Processing Toolkit](https://www.kaggle.com/shivamb/spacy-text-meta-features-knowledge-graphs)\n",
    " \n",
    "- Building a Basic Knowledge Graph using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "85600930b325b2bc99c604b7535cf298e475c247"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56cdec5c70d72fc837d597407ea6582e34e15781"
   },
   "source": [
    "## Basics of Knowledge Graphs using spaCy\n",
    "\n",
    "In this section, I have explained the basics of building knowledge graphs using spaCy. \n",
    "First, lets understand what are knoweldge graphs. \n",
    "\n",
    "- What are knowlege graphs ? \n",
    "> Knowledge stored in a graph form. The knowledge is captured in entities, attributes, relationships. The Nodes represents entities, NodeLabels represents attributes, and Edges represents Relationships. \n",
    "\n",
    "- Example:  \n",
    "> Chris Nolan (Director, Producer, person) ---> born in  ----> London (place) ---> Director of  ----> Interstellar (Movie) ---> shooted in  -----> Iceland (place)  \n",
    "\n",
    "- Source of information for building knowledge graphs: \n",
    "> Structured Text: Wikipedia, Dbpedia  \n",
    "> Unstructured Text: Social Media, Blogs, Images, Videos, Audios \n",
    "\n",
    "#### Main ideas for building knowlege graphs\n",
    "\n",
    "- Entity Extraction   \n",
    "In this step, the aim is to extract right entities from the text data. spaCy provides NER (Named Entity Recognition) which can be used for this purpose.  \n",
    "\n",
    "- Relationship Extraction    \n",
    "In this step, the aim is to identify the relationship between the sentences / entities. Again, by using spaCy one can extract the grammar relations between two words / entities.  \n",
    "\n",
    "- Relationship Linking    \n",
    "The hard part of knowlege graphs is to identify what kind of relationship exists between the two entities. The idea is to add the contextual sense to the relationship. \n",
    "\n",
    "Let's look at very high level implementation of this idea using spacy. Lets load a news dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "7e30860a91ef34a6d07427110f1d6ce64993bf8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300000</td>\n",
       "      <td>World's rarest stamp smashes sale records</td>\n",
       "      <td>http://www.thetimes.co.uk/tto/news/world/ameri...</td>\n",
       "      <td>The Times \\(subscription\\)</td>\n",
       "      <td>e</td>\n",
       "      <td>dv5GXdcteE5TwrMUj7DqIO5xDrWlM</td>\n",
       "      <td>www.thetimes.co.uk</td>\n",
       "      <td>1403086630008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300001</td>\n",
       "      <td>Murderer's Estate Sells Stamp for Record $9.5 ...</td>\n",
       "      <td>http://www.newsmax.com/US/du-Pont-Guyana-stamp...</td>\n",
       "      <td>Newsmax.com</td>\n",
       "      <td>e</td>\n",
       "      <td>dv5GXdcteE5TwrMUj7DqIO5xDrWlM</td>\n",
       "      <td>www.newsmax.com</td>\n",
       "      <td>1403087405702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300002</td>\n",
       "      <td>Stamp sells for record US$9.5m in New York</td>\n",
       "      <td>http://www.businesstimes.com.sg/breaking-news/...</td>\n",
       "      <td>THE BUSINESS TIMES \\(subscription\\)</td>\n",
       "      <td>e</td>\n",
       "      <td>dv5GXdcteE5TwrMUj7DqIO5xDrWlM</td>\n",
       "      <td>www.businesstimes.com.sg</td>\n",
       "      <td>1403087406095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              TITLE  \\\n",
       "0  300000          World's rarest stamp smashes sale records   \n",
       "1  300001  Murderer's Estate Sells Stamp for Record $9.5 ...   \n",
       "2  300002         Stamp sells for record US$9.5m in New York   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  http://www.thetimes.co.uk/tto/news/world/ameri...   \n",
       "1  http://www.newsmax.com/US/du-Pont-Guyana-stamp...   \n",
       "2  http://www.businesstimes.com.sg/breaking-news/...   \n",
       "\n",
       "                             PUBLISHER CATEGORY  \\\n",
       "0           The Times \\(subscription\\)        e   \n",
       "1                          Newsmax.com        e   \n",
       "2  THE BUSINESS TIMES \\(subscription\\)        e   \n",
       "\n",
       "                           STORY                  HOSTNAME      TIMESTAMP  \n",
       "0  dv5GXdcteE5TwrMUj7DqIO5xDrWlM        www.thetimes.co.uk  1403086630008  \n",
       "1  dv5GXdcteE5TwrMUj7DqIO5xDrWlM           www.newsmax.com  1403087405702  \n",
       "2  dv5GXdcteE5TwrMUj7DqIO5xDrWlM  www.businesstimes.com.sg  1403087406095  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/uci-news-aggregator\"\n",
    "files = os.listdir(path)\n",
    "df = pd.read_csv(os.path.join(path,files[0]),nrows=5000)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_title</th>\n",
       "      <th>named_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(World, 's, rarest, stamp, smashes, sale, reco...</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Murderer, 's, Estate, Sells, Stamp, for, Reco...</td>\n",
       "      <td>((Murderer, 's), ($, 9.5, Million))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Stamp, sells, for, record, US$, 9.5, m, in, N...</td>\n",
       "      <td>((US$, 9.5, m), (New, York))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         spacy_title  \\\n",
       "0  (World, 's, rarest, stamp, smashes, sale, reco...   \n",
       "1  (Murderer, 's, Estate, Sells, Stamp, for, Reco...   \n",
       "2  (Stamp, sells, for, record, US$, 9.5, m, in, N...   \n",
       "\n",
       "                        named_entities  \n",
       "0                                   ()  \n",
       "1  ((Murderer, 's), ($, 9.5, Million))  \n",
       "2         ((US$, 9.5, m), (New, York))  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply spacy to the article titles\n",
    "df[\"spacy_title\"] = df[\"TITLE\"].apply(lambda x : nlp(x))\n",
    "\n",
    "# add field of NE\n",
    "df[\"named_entities\"] = df[\"spacy_title\"].apply(lambda x : x.ents)\n",
    "\n",
    "df[['spacy_title','named_entities']][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2099872afd539fa28784a066f2ce0c06c42a2585"
   },
   "source": [
    "## POS Pattern Recognition\n",
    "#### [here is a list of POS](https://sites.google.com/site/partofspeechhelp/home/nnp_nnps#TOC-Definition-of-NNPS-Proper-Noun-Plural-Form-1)\n",
    "Now, we will define a grammar pattern / part of speech pattern to identify what type of relations we want to extract from the data. \n",
    "\n",
    "Let's we are interested in finding an action relation between two named entities. so we can define a pattern using part of speech tags as : \n",
    "\n",
    "Proper Noun - Verb - Proper Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_chain_1 = \"NNP-VBZ-NNP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98847a394b96a3abfc71e8d9ea28ed77f27d4b0a"
   },
   "source": [
    "Using spaCy, we can now iterate in text and identify what are the relevant triplets (governer, relation, dependent) or in other terms, what are the entities and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "fd80efd1e651449ee42847f5859feadea27a15ee"
   },
   "outputs": [],
   "source": [
    "index_list = list()\n",
    "for i, r in df.iterrows():\n",
    "    pos_chain = \"-\".join([d.tag_ for d in r['spacy_title']])\n",
    "    if pos_chain_1 in pos_chain:\n",
    "        if len(r[\"named_entities\"]) == 2:\n",
    "            index_list.append(i)\n",
    "#             print (r[\"TITLE\"])\n",
    "#             print (r[\"named_entities\"])\n",
    "#             print (pos_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Patent Office cancels Washington Redskins trademarks \n",
      "\n",
      "Entity span \"US Patent Office\" [0:3]\n",
      "Entity span \"Washington Redskins\" [4:6]\n"
     ]
    }
   ],
   "source": [
    "words = df.loc[index_list[90]]['spacy_title']\n",
    "print(words,'\\n')\n",
    "for ent in words.ents:\n",
    "    print('Entity span \"{}\" [{}:{}]'.format(ent,ent.start,ent.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'NNP', 'NNP', 'VBZ', 'NNP', 'NNPS', 'NNS']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.tag_ for w in df.loc[index_list[90]]['spacy_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1d96806e871e80c0fc0f9316d33db3714fdedfc"
   },
   "source": [
    "So from these examples, one can see different entities and relations for example: \n",
    "\n",
    "- Honda --- **restructures** ---> US operations  \n",
    "- Carl Icahn --- **slams** ---> eBay CEO\n",
    "- Google --- **confirms** ---> Android SDK \n",
    "- GM --- **hires** ---> Lehman Brothers \n",
    "\n",
    "References : https://kgtutorial.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:training]",
   "language": "python",
   "name": "conda-env-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
