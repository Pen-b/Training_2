{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove single characters\n",
    "data['doc'] = data['doc'].apply(lambda x: re.sub(r'\\b\\w\\b', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "# tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(np.concatenate([x.split() for x in data.doc])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list\n",
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations = pd.read_csv(r'P:\\MyWork\\ner_resources\\country-nationality.txt', encoding='utf-8', sep=',',header=0,\n",
    "                 usecols=[1,2,3,4],names=['c1','c2', 'nm', 'nt'])\n",
    "\n",
    "nat = []\n",
    "for col in nations.columns:\n",
    "    nat.append(list(nations[col].str.lower().unique()))\n",
    "nat = [item for sublist in nat for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities = list(pd.read_csv(r'P:\\MyWork\\ner_resources\\us-cities.txt', encoding='utf-8')['city'].str.lower())\n",
    "us_cities = [x for x in us_cities if isinstance(x, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us = pd.read_csv(r'P:\\MyWork\\ner_resources\\US\\US.txt', encoding='utf-8', sep='\\t', header=None,index_col=None,\n",
    "                 usecols=[0,3,4],names=['c','state', 'sc'])\n",
    "us_places = []\n",
    "for col in us.columns:\n",
    "    us_places.append(list(us[col].str.lower().unique()))\n",
    "us_places = [item for sublist in us_places for item in sublist]\n",
    "us_places = [x for x in us_places if isinstance(x, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_words.union(us_places+us_cities+beaz_stopwords+brokers+nat)\n",
    "stops = [x for x in stop_words if isinstance(x, str)]\n",
    "stops = list(set(stops))\n",
    "stops = ['\\\\b'+re.sub(r'[^a-zA-Z ]+', '', x)+'\\\\b' for x in stops] # add breaks around word for whole word search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['doc_stop'] = data['doc'].str.split().apply(lambda x: [item for item in x if item not in stop_words]).str.join(' ') # only does single stop_words\n",
    "data['doc_stop'] = data['doc'].copy()\n",
    "data['doc_stop'].replace(to_replace=stops, value='', inplace=True, regex=True) # does multi-len stop words e.g. \"north carolina\"\n",
    "\n",
    "data['doc_stop'].replace('\\s+', ' ', regex=True, inplace=True) # remove multi whitespaces\n",
    "data['doc_stop'].replace(' ', np.nan, inplace=True) # replace empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autocorrect import spell\n",
    "# data['doc_stop_spell'] = data['doc_stop'].copy()\n",
    "# data['doc_stop_spell'] = \n",
    "# data['doc_stop_spell'].dropna().str.split().apply(lambda x: [spell(item) for item in x]).str.join(' ')\n",
    "# data[['doc','doc_stop', 'doc_stop_spell']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "data['doc_stop_stem'] = data['doc_stop'].copy()\n",
    "data['doc_stop_stem'] = data['doc_stop_stem'].dropna().str.split().apply(lambda x: [ps.stem(item) for item in x]).str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dat[['uid','sr_section_reference','er_exposure_reference']].merge(data, how='right', left_on='uid', right_on='uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Clustering](https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html)\n",
    "\n",
    "- https://stackoverflow.com/questions/27889873/clustering-text-documents-using-scikit-learn-kmeans-in-python\n",
    "- http://brandonrose.org/clustering\n",
    "- https://pythonprogramminglanguage.com/kmeans-text-clustering/\n",
    "- https://stackoverflow.com/questions/28160335/plot-a-document-tfidf-2d-graph/28205420#28205420\n",
    "- https://stackoverflow.com/questions/36946510/from-text-to-k-means-vectors-input\n",
    "- http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#\n",
    "- http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "- http://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "- http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'storm'\n",
    "print(data['doc_stop_stem'].loc[data['doc_stop_stem'].str.contains(term)==True].count() / len(data))\n",
    "#data['doc_stop_stem'].loc[data['doc_stop_stem'].str.contains(term)==True][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TV = TfidfVectorizer(stop_words=set(stops), ngram_range=(1,3),\n",
    "                     min_df=0.0, max_df=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc idf sparse matrix \n",
    "X = TV.fit_transform(data['doc_stop_stem'].dropna())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_score = {}\n",
    "for k in np.arange(20,120,10):\n",
    "    KM = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=1, n_jobs=-2)\n",
    "    k_score[k] = KM.fit(X).score(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k,v in k_score.items():\n",
    "    #print(k,v)\n",
    "plt.plot(k_score.keys(),k_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=25, init='k-means++', max_iter=300, n_init=1, n_jobs=-2)\n",
    "KM.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhoutte scoring\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# model.labels_.tolist() # equivalnet to predcit(X)\n",
    "silhouette_score(X, KM.labels_, sample_size=10000, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_centroids = KM.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = TV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters={}\n",
    "for i in range(25):\n",
    "    ter = []\n",
    "    print( \"\\nCluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        #print(i,ind)\n",
    "        print( '%d %s' %( ind,terms[ind]), end=',')\n",
    "        ter.append(terms[ind])\n",
    "    clusters[i] = ter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cluster']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['doc_stop_stem'].notnull(),'cluster'] = KM.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA / SVD Dimension reduction \n",
    "\n",
    " - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD.fit_transform\n",
    "\n",
    " - https://stackoverflow.com/questions/42882207/plot-k-means-clusters-after-truncatedsvd-python\n",
    "\n",
    "#### n_components\n",
    " - https://chrisalbon.com/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/\n",
    " - https://stackoverflow.com/questions/48424084/number-of-components-trucated-svd\n",
    " - https://cstheory.stackexchange.com/questions/21487/when-to-use-the-johnson-lindenstrauss-lemma-over-svd/21489#21489\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = TruncatedSVD(n_components=2)\n",
    "data2D = SVD.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data2D[:,0], data2D[:,1],cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nltk]",
   "language": "python",
   "name": "conda-env-nltk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
