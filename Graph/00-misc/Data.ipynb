{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "import fuzzywuzzy as fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "path = r'P:\\MyWork\\product-recomender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [x for x in os.listdir(path+'\\data\\\\raw') if 'extract' in x]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def combine_files(path, files):\n",
    "    output_list = []\n",
    "    \n",
    "    # for loop to read each excel file and extract data sheet. \n",
    "    for file in files:\n",
    "        \n",
    "        print('\\rReading \"%s\"' %file, end='')\n",
    "        wb = pd.read_excel(path+file, sheetname=['Main Page'], skiprows=4, index_col=None)\n",
    "        output_list.append(wb['Main Page'])\n",
    "\n",
    "    print('\\n\\t combining files...')   \n",
    "    # concatentate df's to one\n",
    "    df = pd.concat(output_list)\n",
    "    df.sort_values(by='Policy YOA', ascending=True, inplace=True) # sort values in order of YOA\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print('\\t standardising column names and values...') \n",
    "    # basic cleaning of column names\n",
    "    df.rename(columns=lambda x: x.replace(\" \", \"_\").lower(), inplace=True) #lower case and remove spaces in names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = combine_files(path+'\\data\\\\raw\\\\', files)\n",
    "print(len(dat))\n",
    "dat.dropna(subset=['insured_party', 'department'], inplace=True)\n",
    "print(len(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dat.to_csv(path+'\\data\\\\processed\\\\eb-extracts.txt', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to remove stop words\n",
    "filename = r'P:\\MyWork\\product-recomender\\data\\processed\\stopwords'\n",
    "\n",
    "#Read\n",
    "with open(filename, 'r') as f:\n",
    "    stopwords = [line.rstrip(\"\\n\").replace(\"\\'\", \"\") for line in f]\n",
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def stops(list_of_strings, stopwords):\n",
    "    cleaned = [x for x in list_of_strings if x not in stopwords]\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = dat[['insured_party','department', 'trifocus', 'coverage_name', 'class_of_business']].copy()\n",
    "sam[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = dat[['insured_party','department', 'trifocus', 'coverage_name', 'class_of_business']].copy()\n",
    "for col in sam.columns:\n",
    "    sam[col]= sam[col].str.lower()\n",
    "    sam[col].replace('[^0-9a-zA-Z ]', '', regex=True, inplace=True)\n",
    "sam[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam['insured'] = sam['insured_party'].copy()\n",
    "sam['insured'] = sam['insured'].str.split(' ').apply(stops, stopwords=stopwords)\n",
    "sam['insured'] = sam['insured'].str.strip()\n",
    "sam['num'] = 1\n",
    "sam['idx']= sam.index\n",
    "sam[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syndicate-COB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobs = pd.read_csv(path+'\\\\references\\\\cobs.csv')\n",
    "cobs['tf'] = cobs['TriFocusGroup'].str.extract('(?<={)(.*)(?=})', expand=False).str.lower().copy()\n",
    "cobs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp = cobs.groupby(['tf','SyndicateCOBName']).agg({'tf':'count'})\n",
    "mapp.rename(columns={'tf':'count'}, inplace=True)\n",
    "mapp = pd.DataFrame(mapp.to_records())\n",
    "mapp.rename(columns={'SyndicateCOBName':'syn-tf'}, inplace=True)\n",
    "mapp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam['syn-tf']=np.nan\n",
    "for tf in mapp['tf'].unique():\n",
    "    sam.loc[sam['trifocus']==tf, 'syn-tf'] = mapp.loc[mapp['tf']==tf, 'syn-tf'].unique()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dat = sam.groupby(['insured', 'syn-tf']).agg({'num':'sum'})\n",
    "dat = pd.DataFrame(dat.to_records())\n",
    "dat['insured'].replace('', np.nan, inplace=True)\n",
    "dat.dropna(subset=['insured'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import community\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/49429594/pyhton-pandas-dataframe-to-adjacency-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Matrix\n",
    "col = 'syn-tf'\n",
    "\n",
    "tf = dat[[col]+['insured']]\n",
    "df_merge = tf.merge(tf, on='insured')\n",
    "results = pd.crosstab(df_merge[col+'_x'], df_merge[col+'_y'])\n",
    "\n",
    "node_weights = np.array(np.diagonal(results.values))\n",
    "np.fill_diagonal(results.values, 0)\n",
    "\n",
    "#results.to_csv('P:\\MyWork\\product-recomender\\gephi\\trifocus_matrix.csv')\n",
    "results.loc[:'Cat',:'Cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results.to_csv('dat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "adjacency = np.array(results)\n",
    "G = nx.from_numpy_matrix(adjacency, create_using=nx.Graph()) # create graph\n",
    "\n",
    "labels = list(results.columns)\n",
    "labels = {e: i for (e, i) in enumerate(labels)}\n",
    "G = nx.relabel_nodes(G,labels) # relabel nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nx.degree(G)\n",
    "node_degree= np.array([i[1] for i in d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = G.edges()\n",
    "edge_weights = np.array([G[u][v]['weight'] for u,v in edges])\n",
    "edge_norm = (edge_weights-min(edge_weights))/(max(edge_weights)-min(edge_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = [G.subgraph(c) for c in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "part = community.best_partition(G, resolution=1)\n",
    "parts = [part.get(node) for node in G.nodes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_graph(graph, axis):\n",
    "    deg = nx.degree(graph)\n",
    "    node_degree= np.array([i[1] for i in deg])\n",
    "    edges = graph.edges()\n",
    "    edge_weights = np.array([graph[u][v]['weight'] for u,v in edges])\n",
    "    edge_norm = (edge_weights-min(edge_weights))/(max(edge_weights)-min(edge_weights))\n",
    "    part = community.best_partition(graph, resolution=1)\n",
    "    parts = [part.get(node) for node in graph.nodes()]\n",
    "    \n",
    "    nx.draw(graph, ax=axis,\n",
    "            pos=nx.spring_layout(graph, center=(1,1), k=15),\n",
    "            node_size=node_weights/2, with_labels=True,\n",
    "            cmap = plt.get_cmap('jet'), node_color = parts,\n",
    "            edgelist=edges, width=edge_norm*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1, figsize=(15,15))\n",
    "plot_graph(G, axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine-learning]",
   "language": "python",
   "name": "conda-env-machine-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
