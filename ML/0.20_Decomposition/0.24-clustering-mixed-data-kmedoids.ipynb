{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering on mixed data-types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article focusses on theory, method and practical application of clustering of datasets with mixed datatypes. Many real world datasets include combinations of numerical, ordianl (e.g. small, medium, large), and ordinal (e.g. France, China, India) data features. Whereas many popular clustering algorithms such as Kmeans are suitable only for either numerical or categorical datatypes independantly. Sklearn provides and excellent [introduction to clustering](https://scikit-learn.org/stable/modules/clustering.html#clustering) methods. Lets begin.\n",
    "\n",
    "Cluster analysis (clustering) is the task of assigning sets of objects within a population in such a way that objects in the same group (cluster) are more similar to one another than to those in other clusters. Clustering is a form of unsupervised learning as the number, size and distribution of clusters is unknown a priori.\n",
    "\n",
    "Clustering can be applied to a variety of different problems and domains including: customer segmentation for retail sales and marketing, identifying higher or lower risk groups within [insurance portfolios](https://www.casact.org/pubs/dpp/dpp08/08dpp170.pdf), to finding [storm systems on Jupyter](https://astronomycommunity.nature.com/users/253561-ingo-waldmann/posts/48323-deep-learning-saturn), and even [galaxies far far away](https://arxiv.org/abs/1404.3097).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Case Study: Auto insurance claims [dataset](https://www.kaggle.com/xiaomengsun/car-insurance-claim-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "DATA_PATH = os.path.join(os.getcwd(),'../_data')\n",
    "df = pd.read_csv(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim.csv'),low_memory=False,)\n",
    "\n",
    "# convert object to numerical\n",
    "df[['INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM', 'CLM_AMT',]] = df[['INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM', 'CLM_AMT',]].replace('[^.0-9]', '', regex=True,).astype(float).fillna(0)\n",
    "\n",
    "# clean textual classes\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'O':\n",
    "        df[col] = df[col].str.upper().replace('Z_','',regex=True).replace('[^A-Z]','',regex=True)\n",
    "        \n",
    "data_types = {f:t for f,t in zip(df.columns,df.dtypes)}\n",
    "\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***missing data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OCCUPATION'].fillna('OTHER',inplace=True)\n",
    "for col in ['AGE','YOJ','CAR_AGE']:\n",
    "    df[col].fillna(df[col].mean(),inplace=True)\n",
    "    \n",
    "print(df.isnull().sum()[df.isnull().sum()>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Encoding & Engineering\n",
    "\n",
    "***what features do we have?***\n",
    "Having explored I found this [data dictionary](https://rpubs.com/data_feelings/msda_data621_hw4) and following key definitions:\n",
    "- Bluebook = car re-sale value. \n",
    "- MVR_PTS = [MotorVehicleRecordPoints (MVR) ](https://www.wnins.com/losscontrolbulletins/MVREvaluation.pdf) details an individual’s past driving history indicating violations and accidents over a specified period\n",
    "- TIF = Time In Force / customer lifetime\n",
    "- YOJ = years in job\n",
    "- CLM_FRQ = # of claims in past 5 years\n",
    "- OLDCLAIM = sum $ of claims in past 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy df\n",
    "tdf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_id = ['ID']\n",
    "feat_account = ['KIDSDRIV', 'BIRTH', 'AGE', 'HOMEKIDS', 'YOJ', 'INCOME',\n",
    "                'PARENT1', 'HOME_VAL', 'MSTATUS', 'GENDER', 'EDUCATION', 'OCCUPATION','URBANICITY','TIF',]\n",
    "feat_car = [ 'TRAVTIME', 'CAR_USE','MVR_PTS','BLUEBOOK','CAR_TYPE', 'RED_CAR','REVOKED','CAR_AGE',]\n",
    "feat_claims = ['OLDCLAIM', 'CLM_FREQ', 'CLAIM_FLAG','CLM_AMT',]\n",
    "\n",
    "data_meta = pd.DataFrame(tdf.nunique(),columns=['num'],index=None).sort_values('num').reset_index()\n",
    "data_meta.columns = ['name','num']\n",
    "data_meta[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***transform binary variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for feat in data_meta.loc[data_meta['num']<=12,'name'].values:\n",
    "    tdf[feat] = le.fit_transform(tdf[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDA & Feat Eng\n",
    "Lets go feature by feature and see if we can remove small feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'KIDSDRIV'\n",
    "fig,ax = plt.subplots(1,2,figsize=(4,2),sharey=True)\n",
    "tdf[f].value_counts().plot.bar(ax=ax[0]);\n",
    "tdf.loc[tdf[f]>=2,f] = 2\n",
    "tdf[f].value_counts().plot.bar(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'HOMEKIDS'\n",
    "fig,ax = plt.subplots(1,2,figsize=(4,2),sharey=True)\n",
    "tdf[f].value_counts().plot.bar(ax=ax[0]);\n",
    "tdf.loc[tdf[f]>=3,f] = 3\n",
    "tdf[f].value_counts().plot.bar(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'YOJ'\n",
    "fig,ax = plt.subplots(1,2,figsize=(20,2),sharey=True)\n",
    "tdf[f].value_counts().plot.bar(ax=ax[0]);\n",
    "tdf.loc[tdf[f]>=17,f] = 17\n",
    "tdf[f].value_counts().plot.bar(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'PARENT1'\n",
    "fig,ax = plt.subplots(1,2,figsize=(20,2),sharey=True)\n",
    "tdf[f].value_counts().plot.bar(ax=ax[0]);\n",
    "#tdf.loc[tdf[f]>=17,f] = 17\n",
    "tdf[f].value_counts().plot.bar(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = tdf[feat_account+feat_car+feat_claims].copy()\n",
    "Xy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Similarity\n",
    "\n",
    "[$Gower$ $distance$](https://www.jstor.org/stable/2528823?seq=1) was proposed to measure dissimilarity between subjects with mixed types of variables using the mathematical concept of distance.\n",
    "- [docs](https://rdrr.io/cran/gower/api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "\n",
    "# # Example: to find the most similar record to i=0, in rows i=1...i=100\n",
    "# gower.gower_topn(Xy.iloc[0:1,:], Xy.iloc[1:100,], n = 1)\n",
    "# Xy.iloc[[0,42],:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    gd = np.load(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_gower_distance.npy'))\n",
    "    print('Gower distances loaded from file.')\n",
    "except:\n",
    "    print('Calculating Gower dsitances...5-8 minutes')\n",
    "    %time gd = gower.gower_matrix(Xy[:])\n",
    "    np.save(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_gower_distance.npy'),gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-mediods python implmentation in scikit-learn-extra\n",
    "# https://scikit-learn-extra.readthedocs.io/en/latest/install.html\n",
    "# C++ build tools may be required on windows\n",
    "# https://www.scivision.dev/python-windows-visual-c-14-required/\n",
    "\n",
    "# or k-mediods in pyclustering\n",
    "# https://pypi.org/project/pyclustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from pyclustering.cluster import cluster_visualizer,cluster_visualizer_multidim\n",
    "from pyclustering.cluster.silhouette import silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "n = 10000\n",
    "print(f'contains nans:\\t{np.isnan(gd).any()}')\n",
    "sample = np.nan_to_num(gd[:n,:n])\n",
    "print(f'sample:\\t{n}\\nshape:\\t{sample.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# G = nx.from_numpy_matrix(sample)\n",
    "# edge_list = [i for i in nx.generate_edgelist(G,data=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cluster $k=n$***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate k random medoids\n",
    "# also sets k clusters\n",
    "%time\n",
    "k = 4\n",
    "initial_medoids = np.random.randint(0,1000,size=k)\n",
    "print(f'Initial medoids:\\t{initial_medoids}')\n",
    "      \n",
    "kmedoids_instance = kmedoids(sample,initial_medoids, data_type='distance_matrix')\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedoids_instance.process()\n",
    "clusters = kmedoids_instance.get_clusters()\n",
    "medoids = kmedoids_instance.get_medoids()\n",
    "\n",
    "# score\n",
    "# The silhouette value is a measure of how similar an object\n",
    "# is to its own cluster compared to other clusters\n",
    "score = silhouette(data=sample, clusters=clusters,data_type='distance_matrix').process().get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in clusters: print(f'k={len(k)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cluster using silhouette score to find $max(k)$***\n",
    "- [visualizer seems to work with paierd list only](https://github.com/annoviko/pyclustering/issues/499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search using silhouette score\n",
    "# https://codedocs.xyz/annoviko/pyclustering/classpyclustering_1_1cluster_1_1silhouette_1_1silhouette__ksearch.html\n",
    "from pyclustering.cluster.center_initializer import random_center_initializer\n",
    "from pyclustering.cluster.silhouette import silhouette_ksearch_type, silhouette_ksearch\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    search_instance = pickle.load(os.path.join(DATA_PATH,'car-insurance-claim-data/kmode-search-6-8.pickle'), \"rb\" )\n",
    "except:\n",
    "    search_instance = silhouette_ksearch(sample, kmin=3, kmax=6, algorithm=silhouette_ksearch_type.KMEDOIDS).process()\n",
    "    pickle.dump(search_instance, open(os.path.join(DATA_PATH,'car-insurance-claim-data/kmode-search-6-8.pickle'), \"wb\" ))\n",
    "\n",
    "amount = search_instance.get_amount()\n",
    "scores = search_instance.get_scores()\n",
    "print(\"Scores: '%s'\" % str(scores))\n",
    "\n",
    "# Create instance of K-Medoids algorithm with optimal settings from search\n",
    "initial_medoids = np.random.randint(0,n,size=amount)\n",
    "kmedoids_instance = kmedoids(sample,initial_medoids, data_type='distance_matrix')\n",
    "kmedoids_instance.process()\n",
    "\n",
    "# capture results\n",
    "clusters = kmedoids_instance.get_clusters()\n",
    "medoids = kmedoids_instance.get_medoids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters), sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, n_iter=500)\n",
    "tsne = tsne_model.fit_transform(Xy[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(tsne)\n",
    "\n",
    "tsne_df['cluster'] = np.nan\n",
    "for e,k in enumerate(clusters):\n",
    "    print(e,len(k))\n",
    "    tsne_df.iloc[k,-1] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = tsne_df.groupby('cluster')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group[0], group[1], marker='o', linestyle='', label=name)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CLARANS](https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.clarans import clarans;\n",
    "from pyclustering.utils import timedcall;\n",
    "# from sklearn import datasets\n",
    "\n",
    "# #import iris dataset from sklearn library\n",
    "# iris =  datasets.load_iris();\n",
    "\n",
    "# #get the iris data. It has 4 features, 3 classes and 150 data points.\n",
    "# data = iris.data\n",
    "\n",
    "# \"\"\"!\n",
    "# The pyclustering library clarans implementation requires\n",
    "# list of lists as its input dataset.\n",
    "# Thus we convert the data from numpy array to list.\n",
    "# \"\"\"\n",
    "# data = data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Xy.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a glimpse of dataset\n",
    "print(\"A peek into the dataset : \",data[:4])\n",
    "\n",
    "\n",
    "\"\"\"!\n",
    "@brief Constructor of clustering algorithm CLARANS.\n",
    "@details The higher the value of maxneighbor, the closer is CLARANS to K-Medoids, and the longer is each search of a local minima.\n",
    "@param[in] data: Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "@param[in] number_clusters: amount of clusters that should be allocated.\n",
    "@param[in] numlocal: the number of local minima obtained (amount of iterations for solving the problem).\n",
    "@param[in] maxneighbor: the maximum number of neighbors examined.        \n",
    "\"\"\"\n",
    "clarans_instance = clarans(data, 3, 4, 4);\n",
    "\n",
    "#calls the clarans method 'process' to implement the algortihm\n",
    "(ticks, result) = timedcall(clarans_instance.process);\n",
    "print(\"Execution time : \", ticks, \"\\n\");\n",
    "\n",
    "#returns the clusters \n",
    "clusters = clarans_instance.get_clusters();\n",
    "\n",
    "#returns the mediods \n",
    "medoids = clarans_instance.get_medoids();\n",
    "\n",
    "\n",
    "print(\"Index of the points that are in a cluster : \",clusters)\n",
    "print(\"The target class of each datapoint : \",iris.target)\n",
    "print(\"The index of medoids that algorithm found to be best : \",medoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *References*\n",
    "\n",
    "- https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3\n",
    "- https://medium.com/@rumman1988/clustering-categorical-and-numerical-datatype-using-gower-distance-ab89b3aa90d9\n",
    "- https://www.researchgate.net/post/What_is_the_best_way_for_cluster_analysis_when_you_have_mixed_type_of_data_categorical_and_scale\n",
    "- https://www.google.com/search?client=firefox-b-d&q=python+gower+distance\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html\n",
    "- https://discuss.analyticsvidhya.com/t/clustering-technique-for-mixed-numeric-and-categorical-variables/6753\n",
    "- https://stackoverflow.com/questions/24196897/r-distance-matrix-and-clustering-for-mixed-and-large-dataset\n",
    "- https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/\n",
    "- https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\n",
    "- https://rpubs.com/data_feelings/msda_data621_hw4\n",
    "- https://pypi.org/project/gower/\n",
    "- https://scikit-learn-extra.readthedocs.io/en/latest/generated/sklearn_extra.cluster.KMedoids.html\n",
    "- https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05\n",
    "- https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/pam\n",
    "- https://github.com/annoviko/pyclustering/issues/499\n",
    "- https://stats.stackexchange.com/questions/2717/clustering-with-a-distance-matrix\n",
    "- https://www.kaggle.com/fabiendaniel/customer-segmentation\n",
    "- https://dkopczyk.quantee.co.uk/claim-prediction/\n",
    "- https://www.casact.org/pubs/dpp/dpp08/08dpp170.pdf\n",
    "- https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4\n",
    "- https://www.uio.no/studier/emner/matnat/math/nedlagte-emner/STK2510/v08/undervisningsmateriale/ch8b.pdf\n",
    "- https://github.com/annoviko/pyclustering/issues/499\n",
    "- https://stackoverflow.com/questions/3081066/what-techniques-exists-in-r-to-visualize-a-distance-matrix\n",
    "- https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n",
    "- https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\n",
    "- http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf\n",
    "- https://www.researchgate.net/post/What_is_the_best_way_for_cluster_analysis_when_you_have_mixed_type_of_data_categorical_and_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine-learning]",
   "language": "python",
   "name": "conda-env-machine-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
