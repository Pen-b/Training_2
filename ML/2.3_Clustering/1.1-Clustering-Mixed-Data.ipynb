{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gower\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['grid.color'] = 'k'\n",
    "mpl.rcParams['grid.linestyle'] = ':'\n",
    "mpl.rcParams['grid.linewidth'] = 0.5\n",
    "mpl.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A guide to clustering large datasets with mixed data-types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pre-note** If you are an early stage or aspiring data analyst, data scientist, or just love working with numbers clustering is a fantastic topic to start with. In fact I actively steer my early career and junior data scientist toward this topic early on in their training and continued professional development cycle - sorry guys but its for your own good.*\n",
    "\n",
    "*Learning how to apply and perform accurate clustering analysis takes you though many of the core principles of data analysis, mathematics, machine learning, and computational science. From learning about data types and geometry, confusion matrix, to applying iterative aglorithms and efficient computation on big data. These foundational concepts crop up in most other areas of data science and machine learning. For instance, cluster distance matrices underpin and, mathematically, are near identical to graph data structures used in deep learning graph neural networks at the cutting edge of artificial intelligence research. So if you are just starting out then dont be put of and read on regardless of your level. We all have to start somewhere and this is a very good place!*\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "Cluster analysis is the task of grouping objects within a population in such a way that objects in the same group or cluster are more similar to one another than to those in other clusters. Clustering is a form of unsupervised learning as the number, size and distribution of clusters is unknown a priori.\n",
    "Clustering can be applied to a variety of different problems and domains including: customer segmentation for retail sales and marketing, identifying higher or lower risk groups within [insurance portfolios](https://www.casact.org/pubs/dpp/dpp08/08dpp170.pdf), to finding [storm systems on Jupyter](https://astronomycommunity.nature.com/users/253561-ingo-waldmann/posts/48323-deep-learning-saturn), and even identifying [galaxies far far away](https://arxiv.org/abs/1404.3097).\n",
    "\n",
    "Many real world datasets include combinations of numerical, ordinal (e.g. small, medium, large), and nomial (e.g. France, China, India) data features. However, many popular clustering algorithms and tutorials such as K-means are suitable for numerical data types only. This article is written on the assumption that these methods are familiar - but otherwise Sklearn provides an excellent review of these methods [here](https://scikit-learn.org/stable/modules/clustering.html#clustering) for a quick refresher. \n",
    "\n",
    "This article seeks to provide a review of methods and a practical application for clustering a dataset with mixed datatypes. You can find all of my code on [Github here](https://github.com/bpostance/training.data_science/blob/master/ML/2.3_Clustering/10-Clustering-Mixed-Data.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Aim:\n",
    "To evaluate methods to cluster datasets containing a variety of dataype's.\n",
    "\n",
    "## 1.2 Objectives:\n",
    "1. To research and review clustering techniques for mixed datatype datasets. \n",
    "1. To research and review feature encoding and engineering strategies. \n",
    "1. To apply and review clustering methods on a test dataset.\n",
    "\n",
    "##  1.3 Case Study: auto-insurance claims\n",
    "The California auto-insurance claims [dataset](https://www.kaggle.com/xiaomengsun/car-insurance-claim-data) contains 8631 observations with two dependent predictor variables Claim Occured and Claim Amount, and 23 independent predictor variables. The [data dictionary](https://rpubs.com/data_feelings/msda_data621_hw4) describe each variable including:\n",
    "- Bluebook = car re-sale value. \n",
    "- MVR_PTS = [MotorVehicleRecordPoints (MVR) ](https://www.wnins.com/losscontrolbulletins/MVREvaluation.pdf) details an individualâ€™s past driving history indicating violations and accidents over a specified period\n",
    "- TIF = Time In Force / customer lifetime\n",
    "- YOJ = years in job\n",
    "- CLM_FRQ = # of claims in past 5 years\n",
    "- OLDCLAIM = sum $ of claims in past 5 years\n",
    "\n",
    "\n",
    "    - https://community.alteryx.com/t5/Alteryx-Designer-Discussions/Insurance-Datasets/td-p/440035\n",
    "    - https://rpubs.com/data_feelings/msda_data621_hw4\n",
    "    - https://rdrr.io/cran/HDtweedie/man/auto.html\n",
    "    - https://cran.r-project.org/web/packages/insuranceData/insuranceData.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>KIDSDRIV</th>\n",
       "      <th>BIRTH</th>\n",
       "      <th>AGE</th>\n",
       "      <th>HOMEKIDS</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>PARENT1</th>\n",
       "      <th>HOME_VAL</th>\n",
       "      <th>MSTATUS</th>\n",
       "      <th>...</th>\n",
       "      <th>CAR_TYPE</th>\n",
       "      <th>RED_CAR</th>\n",
       "      <th>OLDCLAIM</th>\n",
       "      <th>CLM_FREQ</th>\n",
       "      <th>REVOKED</th>\n",
       "      <th>MVR_PTS</th>\n",
       "      <th>CLM_AMT</th>\n",
       "      <th>CAR_AGE</th>\n",
       "      <th>CLAIM_FLAG</th>\n",
       "      <th>URBANICITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63581743</td>\n",
       "      <td>0</td>\n",
       "      <td>MAR</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67349.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>...</td>\n",
       "      <td>MINIVAN</td>\n",
       "      <td>YES</td>\n",
       "      <td>4461.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>HIGHLYURBANURBAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132761049</td>\n",
       "      <td>0</td>\n",
       "      <td>JAN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>91449.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>257252.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>...</td>\n",
       "      <td>MINIVAN</td>\n",
       "      <td>YES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>HIGHLYURBANURBAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>921317019</td>\n",
       "      <td>0</td>\n",
       "      <td>NOV</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>52881.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>...</td>\n",
       "      <td>VAN</td>\n",
       "      <td>YES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>HIGHLYURBANURBAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>727598473</td>\n",
       "      <td>0</td>\n",
       "      <td>MAR</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16039.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>124191.0</td>\n",
       "      <td>YES</td>\n",
       "      <td>...</td>\n",
       "      <td>SUV</td>\n",
       "      <td>NO</td>\n",
       "      <td>38690.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>HIGHLYURBANURBAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450221861</td>\n",
       "      <td>0</td>\n",
       "      <td>JUN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>306251.0</td>\n",
       "      <td>YES</td>\n",
       "      <td>...</td>\n",
       "      <td>MINIVAN</td>\n",
       "      <td>YES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>HIGHLYURBANURBAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  KIDSDRIV BIRTH   AGE  HOMEKIDS   YOJ   INCOME PARENT1  HOME_VAL  \\\n",
       "0   63581743         0   MAR  60.0         0  11.0  67349.0      NO       0.0   \n",
       "1  132761049         0   JAN  43.0         0  11.0  91449.0      NO  257252.0   \n",
       "2  921317019         0   NOV  48.0         0  11.0  52881.0      NO       0.0   \n",
       "3  727598473         0   MAR  35.0         1  10.0  16039.0      NO  124191.0   \n",
       "4  450221861         0   JUN  51.0         0  14.0      0.0      NO  306251.0   \n",
       "\n",
       "  MSTATUS  ... CAR_TYPE RED_CAR OLDCLAIM  CLM_FREQ REVOKED  MVR_PTS  CLM_AMT  \\\n",
       "0      NO  ...  MINIVAN     YES   4461.0         2      NO        3      0.0   \n",
       "1      NO  ...  MINIVAN     YES      0.0         0      NO        0      0.0   \n",
       "2      NO  ...      VAN     YES      0.0         0      NO        2      0.0   \n",
       "3     YES  ...      SUV      NO  38690.0         2      NO        3      0.0   \n",
       "4     YES  ...  MINIVAN     YES      0.0         0      NO        0      0.0   \n",
       "\n",
       "  CAR_AGE CLAIM_FLAG        URBANICITY  \n",
       "0    18.0          0  HIGHLYURBANURBAN  \n",
       "1     1.0          0  HIGHLYURBANURBAN  \n",
       "2    10.0          0  HIGHLYURBANURBAN  \n",
       "3    10.0          0  HIGHLYURBANURBAN  \n",
       "4     6.0          0  HIGHLYURBANURBAN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "DATA_PATH = os.path.join(os.getcwd(),'../_data')\n",
    "df = pd.read_csv(os.path.join(DATA_PATH,'car_insurance_claim.csv'),low_memory=False,)\n",
    "\n",
    "# convert object to numerical\n",
    "df[['INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM', 'CLM_AMT',]] = df[['INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM', 'CLM_AMT',]].replace('[^.0-9]', '', regex=True,).astype(float).fillna(0)\n",
    "\n",
    "# clean textual classes\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'O':\n",
    "        df[col] = df[col].str.upper().replace('Z_','',regex=True).replace('[^A-Z]','',regex=True)\n",
    "        \n",
    "data_types = {f:t for f,t in zip(df.columns,df.dtypes)}\n",
    "\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Method\n",
    "\n",
    "## 3.1 Data pre-processing\n",
    "Apply processing to correct and handle erroneous values, and rename fields and values to make the data easier to work with. Including:\n",
    " - remove or fill null values\n",
    " - drop irrelevant columns\n",
    " - shorten categorical value names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs\n"
     ]
    }
   ],
   "source": [
    "# copy df\n",
    "tdf = df.copy()\n",
    "\n",
    "# drop ID and Birth\n",
    "tdf.drop(labels=['ID','BIRTH'],axis=1,inplace=True)\n",
    "\n",
    "# remove all nan values\n",
    "tdf['OCCUPATION'].fillna('OTHER',inplace=True)\n",
    "for col in ['AGE','YOJ','CAR_AGE']:\n",
    "    tdf[col].fillna(tdf[col].mean(),inplace=True)\n",
    "if tdf.isnull().sum().sum() == 0: print('No NaNs')\n",
    "    \n",
    "data_meta = pd.DataFrame(tdf.nunique(),columns=['num'],index=None).sort_values('num').reset_index()\n",
    "data_meta.columns = ['name','num']\n",
    "data_meta['type'] = 'numerical'\n",
    "\n",
    "# exclude known numericals\n",
    "data_meta.loc[(data_meta['num']<=15) & (~data_meta['name'].isin(['MVR_PTS','CLM_FREQ','CLAIM_FLAG'])),'type']='categorical'\n",
    "data_meta.loc[data_meta['name'].isin(['CLM_FREQ','CLAIM_FLAG']),'type']='claim'\n",
    "\n",
    "cat_features = list(data_meta.loc[data_meta['type']=='categorical','name'])\n",
    "num_features = list(data_meta.loc[data_meta['type']=='numerical','name'])\n",
    "\n",
    "\n",
    "# shorten names\n",
    "tdf['URBANICITY'] = tdf['URBANICITY'].map({'HIGHLYURBANURBAN':'URBAN',\n",
    "                                           'HIGHLYRURALRURAL':'RURAL'})\n",
    "tdf['EDUCATION'] = tdf['EDUCATION'].map({'HIGHSCHOOL':'HSCL',\n",
    "                                         'BACHELORS':'BSC',\n",
    "                                         'MASTERS':'MSC',\n",
    "                                         'PHD':'PHD'})\n",
    "tdf['CAR_TYPE'] = tdf['CAR_TYPE'].map({'MINIVAN':'MVAN', \n",
    "                                       'VAN':'VAN', \n",
    "                                       'SUV':'SUV',\n",
    "                                       'SPORTSCAR':'SPRT',\n",
    "                                       'PANELTRUCK':'PTRK', \n",
    "                                       'PICKUP':'PKUP'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mosaic Plots\n",
    "# # https://rpubs.com/data_feelings/msda_data621_hw4\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "# props = {}\n",
    "# for car in tdf['CAR_TYPE'].unique():\n",
    "#     for i,color in zip([0,1],['grey','red']):\n",
    "#         props[(str(i),car)] = {'color':color}\n",
    "# props\n",
    "\n",
    "# m = mosaic(tdf, ['CAR_TYPE','CLAIM_FLAG',], title='DataFrame as Source',properties=props)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Categorical feature histograms***\n",
    "\n",
    "Shown below are the histogram of each categorical feature. This illustrates both the number and frequency of each category in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(3,4,figsize=(12,9),sharey=True)\n",
    "\n",
    "for ax,feat in zip(axs.flatten(),cat_features):\n",
    "    ax.hist(tdf[feat],align='left')\n",
    "    ax.set_title(feat)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How are claims distributed amongst the categorical features?***\n",
    "\n",
    "As above, the bar plots again illustrate each categorical feature and value, but now also show how the proportion of claims is distributed to each categorical value. For example, Commericial CAR_USE has a relatively higher proportion of claims than Private car use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(3,4,figsize=(12,10),sharey=True)\n",
    "\n",
    "for ax,feat in zip(axs.flatten(),cat_features):\n",
    "    ((pd.crosstab(tdf['CLAIM_FLAG'],tdf[feat])) / (pd.crosstab(tdf['CLAIM_FLAG'],tdf[feat]).sum())).T.plot.bar(stacked=True,ax=ax,legend=False,title=None)\n",
    "    ax.set_title(feat)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Clustering\n",
    "\n",
    "Recall that each clustering algorithm is an attempt to create natural groupings of the data. At a high-level, clustering algorithms acheive this using a measure of similarity or distance between each pair of data points, between groups and partitions of points, or between points and groups to a representative central point (i.e. centroid). So while the actual algorithm impementations to achive this vary, in essence they are based on this simple principle of distance.This is illustrated quite nicely in illustration below that shows a data set with 3 clusters, and iterative cluster partitioning a-f by updating the centroid points (Chen 2018). Here the clusters are formed by measuring the distance between each data point (solid fill) and a representative centoid point (hollow fill). \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Yu-Zhong-Chen/publication/324073652/figure/fig2/AS:611048927277056@1522696825062/A-schematic-illustration-of-the-K-means-algorithm-for-two-dimensional-data-clustering.png\" width=650 height=650 />\n",
    "\n",
    "Because clustering algortithms utilise this concept of distance both it is crucial to consider both:\n",
    "- Distance Meaasures. The distance or \"similarity\" measure used.\n",
    "- Feature Engineering. The nature of our data and the way the data is presented the clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Distance Measures\n",
    "\n",
    "Below are some of the common distance measures used for clustering. Computational efficiency is important here as each data feature introduces an additional dimension.For clustering, by definition, we often have a multiple features to make sense of and therefore the efficiency of the calculation in high dimensional space is crucial. \n",
    "\n",
    "***[Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance#Higher_dimensions)*** is the absolute numerical difference of their location in euclidean space. Distances can be 0 or take on any positive real number. It is given by the root sum-of-squares of differences between each pair (p,q) of points. And we can see that for high dimensions we simply add the distance. \n",
    "\n",
    "$$d_n(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2...+(p_n-q_n)^2}$$\n",
    "\n",
    "***[Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry)*** is again the sum of the absolute numerical difference between two points in space, but using cartesian cooridantes. Whilst euclidean distance is the straight line \"as the crow flies\" with pythagorus theorem, Manhattan takes distance as the sum of the line vectors (p,q).\n",
    "\n",
    "$$d_n(p,q)  = \\sum_{i=1}^{n} |{(p_n-q_n)}|$$\n",
    "\n",
    "This image illustrates examples for: a) euclidean space distance, b) Manhattan distance in cartesian cooridate space, and c) both with the green line showing a euclidean path, while the blue, red, and yellow lines take a cartesian path with Manhattan distance. This illustrates how clustering results may be influenced by distance measures applied and depending on wether the data features are real and numeric or discrete ordinal and categorical values. In addition, perhaps this also illustrates to you how and why geometry and distance are important in other domains such as shortest path problems. \n",
    "<img src=\"spaces.png\" width=750 height=450 />\n",
    "\n",
    "There are many distance metrics (e.g. see [these slides](http://halweb.uc3m.es/esp/Personal/personas/jmmarin/esp/MetQ/Talk6.pdf)). Minkowski distance for example, is a generalization of both the Euclidean distance and the Manhattan distance. Scipy has a covenient [pair distance](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.spatial.distance.pdist.html) ```pdist()``` function that applies many of the most common measures. \n",
    "\n",
    "There are also hybrid distance measures. In our case study, and topic of this article, the data contains a mixture of features with different data types and this requires such a measure.\n",
    "\n",
    "***[Gower (1971) distance](https://www.jstor.org/stable/2528823?seq)*** is a hybrid measure that handles both continuous and categorical data. \n",
    "- If the data feature are continuous or ordinal, the Manhattan or a ranked ordinal Manhattan is applied respectively. \n",
    "- If the data feature are categorical, then a [DICE](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient#Formula) coefficient is applied. DICE is explained [here](https://stats.stackexchange.com/a/55802/100439). However, If you are familiar with Jaccard coefficient and or binary classification (e.g. True Positives TP and False Posititves FP etc) and confusion matrices then DICE is going to be familiar as\n",
    "\n",
    "$$DICE = \\frac{2|X \\cap Y|}{|X|+|Y|} = \\frac{2TP}{2TP+FP+FN}$$\n",
    "\n",
    "The Gower distance of a pair of points $G(p,q)$ then is:\n",
    "\n",
    "$$G_n(p,q) = \\frac{\\sum_{i=1}^{n}W_{pqk}S_{pqk}}{\\sum_{i=1}^{n}W_{pqk}}$$\n",
    "\n",
    "where $S_{pqk}$ is either the Manhattan or DICE value for feature $k$, and $W_{pqk}$ is either 1 or 0 if $k$ feature is valid. Its the sum of feature scores divided by the sum of feature weights. \n",
    " \n",
    "Gower is implemented in [Gower Python](https://www.thinkdatascience.com/post/2019-12-16-introducing-python-package-gower/) and in [Gower R](https://rdrr.io/cran/gower/api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Feature Engineering\n",
    "\n",
    "With this improved understanding of the clustering distance measures it is now clear that the scale of our data features is equally important. For example, imagine we were clustering cars by their performance and weight characteristics. \n",
    "- Car A does 0-60 mph in 3.5 seconds and has mass 3000 KG\n",
    "- Car B does 0-60 mph in 5.5 seconds and has mass 5000 KG\n",
    "\n",
    "The feature distances between Car A and B are 2.0 seconds and 2000 KG. Thus our 2000 unit distance for mass is orders of magnitude higher than 2.0 seconds for 0-60 mph. Clustering data in this form would yield results bias toward high range features (see more examples in these StackOverflow answers [1](https://datascience.stackexchange.com/questions/22795/do-clustering-algorithms-need-feature-scaling-in-the-pre-processing-stage/22915),[2](https://stats.stackexchange.com/a/7182/100439),[3](https://stats.stackexchange.com/questions/385775/normalizing-vs-scaling-before-pca)). When using any algorithm that computes distance or assumes normally distributed data, scale your features [4](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "For datasets with mixed data-types consider you have scaled all features to between 0-1. This will ensure distance measures are applied uniformly to each feature. The numerical features will have distances with min-max 0-1 and real numbers between e.g. 0.1,0.2,0.5,...0.99. Whereas the distances for categorical feautures be values of either 0 or 1. As for mass KG in the car example above, this could still lead to a bias in the formation of clusters toward categorical feature groups as their distances are always either the min-max value of 0 or 1.\n",
    "\n",
    "Selecting the appropriate transformations and scaling to apply is part science and part art. There are often several strategies that may suit and must be applied and evalauted in relation to the context of challenge at hand, the data and its domain. Crucially, whatever strategy is adopted,  **all features** in the final dataset that is used for clustering data must be on the same scale for each eature to be treated equally by the distance metrics. \n",
    "\n",
    "\n",
    "***Different data-types and how to handle them***\n",
    "\n",
    "Here are two excellent articles and figures that delve deeper on (left) [data-types](https://towardsdatascience.com/data-types-in-statistics-347e152e8bee) and (right) [encoding techniques](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02). \n",
    "\n",
    "<p align=\"middle\">\n",
    "  <img src=\"https://miro.medium.com/max/700/1*dvvxoZTdewLFs3RyZTJreA.png\" width=\"30%\"/>\n",
    "  <img src=\"https://innovation.alteryx.com/content/images/2019/08/categorical-encoding-01-01.png\" width=\"30%\" />\n",
    "</p>\n",
    "\n",
    "In summary these are:\n",
    "\n",
    "***Numerical features:*** continous and interval features such as mass, age, speed, luminosity, friction. \n",
    " - Use ratios and percentages.\n",
    " - normalisation (i.e. normalise values to be on scale of 0-1)\n",
    " - standardisation (i.e. how many standard deviations the value is from the sample mean)\n",
    " - transformation (i.e. log transformation).\n",
    "\n",
    "\n",
    "***Nominal Categorical features:*** Unordered nomial or binary symmetric values where outcomes are of equal importance (e.g. Male or Female).\n",
    " - One-hot and dummy encoding (i.e. create binary indicator of each category).\n",
    " - If handling a feature with high cardinality >15, try to reduce dimensionality by feature engineering or apply binary or hash encoding. \n",
    " \n",
    " \n",
    "***Ordinal Categorical features:*** Ordered ordinal or binary asymmetric values, where outcomes are not of equal importance.\n",
    ">**Example**: consider the ordinal categorical feautre for olmypic medals encoded as Bronze=1, Silver=2, Gold=3. Here bronze (= 1 * 1), silver (= 2 * bronze), gold (= 3 * bronze). The difference between Gold-Bronze (= 3-1 = 2) is greater than between Gold and Silver (= 3-2 = 1) worse. Clearly this is ordinal as each category has a rank and relative scale comapred to each other category.\n",
    " - Label encoding with 0-1 normalisation if values are of equal-importance and on increasing scale. \n",
    "  - Rank values with 0-1 normalisation, again if values are on equal-importance increasing scale.\n",
    " - If there is a binary target variable in the dataset (e.g. event occurence, medical diagnosis, iris type), one can also assign frequencies, odd ratios, or weights-of-evidence to each ordinal class.\n",
    "\n",
    "By far ordinal data is the most challenging to handle. There are many arguments between mathmatical purists, statisticians and other data practitioners on wether to treat ordinal data as qualitatively or quantitatively ([see here](https://creativemaths.net/blog/ordinal/)).\n",
    "\n",
    "\n",
    ">**NOTE**: Distance measures and encoding is perhaps THE hardest part to clustering data. In this authors opinion, especially for categorical and ordinal features, data should be treated with caution and practitioners should trial and evaluate different feature engineering and encdoing strategies based on their understanding of the data and its domain. My other recommendation is to trial and test different strategies first with smaller datasets and those where you have a reasonable understanding of what the clusters SHOULD be (Iris, Titanic). Valid your approach on these data, and then apply to you own data and build up the complexity from there deciding where to put more focus on tuning the encoding process. Here are some more useful links: <br>\n",
    "> - [Binary symmetric and assymetric variables](https://www.quora.com/What-are-binary-symmetric-and-asymmetric-attributes)\n",
    "> - [datatype conversions in clustering](https://paginas.fe.up.pt/~ec/files_0506/slides/05_Clustering.pdf)\n",
    "> - [Normalization vs Standardization â€” Quantitative analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)\n",
    "> - [Normalization vs Standardization](https://stats.stackexchange.com/a/10291/100439)\n",
    "\n",
    "\n",
    "### 3.3.3 Clustering Methods\n",
    "There are of course also considertions for the clustering algorithim applied. These will be introduced and discussed on application to the case study dataset in the results section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale,RobustScaler,StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale standardisation of numerical values\n",
    "numerical_features = pd.DataFrame(StandardScaler().fit_transform(tdf[num_features]),index=tdf.index,columns=num_features)\n",
    "numerical_features = pd.DataFrame(MinMaxScaler(feature_range=(0,1)).fit_transform(tdf[num_features]),index=tdf.index,columns=num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Categorical:Nomial values with binary symmetry***\n",
    "\n",
    "Remember, here we are taking categorical values that are symmetric in scale only. Without getting into a debate, we could consider EDUCATION and OCCUPATIOM as either nomial (i.e. no order) or ordinal (i.e. hierachal). Here i'm going to take education as ordinal and occupation as nomial.\n",
    "\n",
    "Observing our plots above we may also want to combine some classes where there are low frequencies or high cardinality. These are:\n",
    "\n",
    " - KIDSDRIV: collapse >= 2 to single category\n",
    " - HOMEKIDS: collapse >= 4 to single category\n",
    "\n",
    "We then apply one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features.remove('EDUCATION')\n",
    "tdf['KIDSDRIV'] = tdf['KIDSDRIV'].map({0:'0',1:'1',2:'2+',3:'2+',4:'2+'})\n",
    "tdf['HOMEKIDS']= tdf['HOMEKIDS'].map({0:'0',1:'1',2:'2',3:'3',4:'4+',5:'4+'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomial_features = pd.get_dummies(tdf[cat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Categorical:Ordinal values with binary asymmetry***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['EDUCATION'] = tdf['EDUCATION'].map({'HSCL':0, 'BSC':1, 'MSC':2,'PHD':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mx = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = pd.DataFrame(mx.fit_transform(tdf[['EDUCATION']]),index=tdf.index,columns=['EDUCATION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create datasets for clustering***\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Beware! check what transformations each package applies or can handle. For example, some may require features to be prepared as above a priori whilst others may handles this for you.\n",
    "</span>\n",
    "\n",
    "Rule of thumb, when using any algorithm that computes distance or assumes normality, scale your features! [see here](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e) and [here](https://stats.stackexchange.com/questions/385775/normalizing-vs-scaling-before-pca).\n",
    "\n",
    "I will create two copies of the data:\n",
    " 1. using the above OHE transformations and feature scaling (0,1).\n",
    " 2. applying the above transformations but without feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed and scaled dataset\n",
    "Xy_scaled = pd.concat([numerical_features,nomial_features,ordinal_features],axis=1)\n",
    "print(f'Data min:max {Xy_scaled.min().min(),Xy_scaled.max().max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "Xy_original = tdf.drop(labels=['CLAIM_FLAG'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOWER\n",
    "\n",
    "Gower is normalising for us see this line:https://github.com/wwwjk366/gower/blob/master/gower/gower_dist.py#L61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Gower to find similarity between a single entity and a list of candidates. \n",
    "\n",
    "This seems to pick out Urban, 40-50 year old males, who drive red minivan's, no kids, and with home values of around $160 K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = gower.gower_topn(Xy_original.iloc[4:5,:], Xy_original.iloc[:,:], n = 10)\n",
    "print(Xy_original.iloc[sd['index']].describe().loc[['mean']])\n",
    "Xy_original.iloc[sd['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to calculate a matrix of similaritities between all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of cat_feature indicator\n",
    "# [(x,True) if x in cat_features else (x,False) for x in Xy_original.columns]\n",
    "cat_ind = [True if x in cat_features else False for x in Xy_original.columns]\n",
    "\n",
    "try: \n",
    "    gd = np.load(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_gower_distance.npy'))\n",
    "    print('Gower distances loaded from file.')\n",
    "except:\n",
    "    print('Calculating Gower dsitances...1-5 minutes')\n",
    "    %time gd = gower.gower_matrix(Xy_original, cat_features=cat_ind)\n",
    "    np.save(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_gower_distance.npy'),gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gd[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Clustering methods for mixed datatypes\n",
    "\n",
    "1. Hierachal (Gower distance matrix from original features):\n",
    " - [see here](https://stackoverflow.com/a/55306715/4538066) for discusion of suitable distance methods.\n",
    "1. K-medoids (transformed and scaled features):\n",
    " - [distance metrics for k-medoids](https://stats.stackexchange.com/a/94178/100439)\n",
    " - [k-mediods in pyclustering package](https://pypi.org/project/pyclustering/)\n",
    "    - [ISSUE](https://github.com/annoviko/pyclustering/issues/503) pyclustering package does not implement PAM as suggested on other sites.\n",
    " - K-medoids is poor performing on large datasets.\n",
    " - [k-mediods python implmentation in scikit-learn-extra](https://scikit-learn-extra.readthedocs.io/en/latest/install.html)\n",
    " - [C++ build tools may be required on windows](https://www.scivision.dev/python-windows-visual-c-14-required/)\n",
    "1. CLARANS (transformed and scaled features)\n",
    " - [Raymond, T., et al. 2002. CLARANS](http://www.cs.ecu.edu/dingq/CSCI6905/readings/CLARANS.pdf)\n",
    " - [clarans in python](https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4)\n",
    "1. PAM partition-around-medoids (transformed and scaled features)\n",
    " - [PAM  is a variation of K-medoids](https://stats.stackexchange.com/a/141208/100439)\n",
    " - [Self defined PAM k-medoids in python](https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05)\n",
    "1. K-means (transformed and scaled features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Hierachal (Gower distance matrix from original features)\n",
    "\n",
    "[scipy.linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage) is used to generate:\n",
    " - $Z$, an ($n-1$) by 4 matrix . \n",
    " - At the -th iteration, clusters with indices $Z[i, 0]$ and $Z[i, 1]$ are combined to form cluster.\n",
    " - A cluster with an index less than corresponds to one of the original observations. \n",
    " - The distance between clusters $Z[i, 0]$ and $Z[i, 1]$ is given by $Z[i, 2]$. \n",
    " - The fourth value $Z[i, 3]$ represents the number of original observations in the newly formed cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output matrix has format [idx1, idx2, dist, sample_count]\n",
    "try: \n",
    "    Z = np.load(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_linkage-complete.npy'))\n",
    "    print('Z linkages loaded from file.')\n",
    "except:\n",
    "    print('Calculating Gower dsitances...1-5 minutes')\n",
    "    %time Z = linkage(gd,method='complete')\n",
    "    np.save(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_linkage-complete.npy'),Z)\n",
    "    \n",
    "Z_df = pd.DataFrame(Z,columns=['id1','id2','dist','n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise using a [scipy.dendogram](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram)\n",
    "\n",
    "[*note*](https://stackoverflow.com/questions/9838861/scipy-linkage-format) it is not practical to infer the cluster or each observation using linkage and dendogram ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(25,5))\n",
    "dn = dendrogram(Z, truncate_mode='level',p=6,show_leaf_counts=True,ax=axs);\n",
    "print(f\"Leaves = {len(dn['leaves'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the optimal number of clusters we apply:\n",
    "1. [fcluster](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster) to form flat clusters from the hierarchical clustering defined by the linkage matrix ($Zd$).\n",
    "1. [Silhouette scoring](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) to determine an appropriate number of clusters ($k$) or level in the dendogram. The Silhouette Coefficient ($S$) is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample where:\n",
    "$$S = (b - a) / max(a, b)$$\n",
    " - ($S_1$) is computed using the \"precomputed\" Gower distances.\n",
    " - ($S_2$) is computed using predefined distance measures from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#) or [scipy](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html), in this instance \"correlation\". BUT, this only works on our transformed and scaled features created earlier.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find k clusters\n",
    "results = dict()\n",
    "\n",
    "k_cand = [2,3,4,5,7,9,]\n",
    "k_cand.extend(list(np.arange(10,55,5)))\n",
    "\n",
    "for k in k_cand:\n",
    "    cluster_array = fcluster(Z, k, criterion='maxclust')\n",
    "    score1 = silhouette_score(gd, cluster_array, metric='precomputed')\n",
    "    score2 = silhouette_score(Xy_scaled, cluster_array,metric='correlation')\n",
    "    results[k] = {'k':cluster_array,'s1':score1,'s2':score2}\n",
    "    \n",
    "plt.plot([i for i in results.keys()],[i['s1'] for i in results.values()],label='gower')\n",
    "plt.plot([i for i in results.keys()],[i['s2'] for i in results.values()],label='correlation')\n",
    "plt.legend()\n",
    "plt.xticks(k_cand);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the silhouette scores indicates that there are increases \"spikes\" in scores at 7 and 40 clusters.\n",
    "Lets investigate these two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 7 and 40\n",
    "tdf['k-medoids-7'] =results[7]['k']\n",
    "tdf['k-medoids-40'] =results[40]['k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(6,2,figsize=(10,15),sharex=True)\n",
    "\n",
    "for ax,feat in zip(axs.flatten(),num_features):\n",
    "    pd.plotting.boxplot(tdf,column=[feat],by='k-medoids-7',ax=ax)\n",
    "    ax.set_xlabel('')  \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(11,1,figsize=(10,40),sharex=True)\n",
    "\n",
    "for ax,feat in zip(axs.flatten(),num_features):\n",
    "    pd.plotting.boxplot(tdf,column=[feat],by='k-medoids-40',ax=ax)\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-medoids can be caluclated using many distance metrics. Here the *Minkowski distance* or \"cityblock\" is used as this provides a suitable measure where there are both categorical and numerical features [see here](https://www2.cs.duke.edu/courses/spring18/compsci216/lectures/07-clustering.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    Xy_scaled_minkowski = np.load(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_Xy_scaled_minkowski.npy'))\n",
    "    print('Minkowski distances loaded from file.')\n",
    "except:\n",
    "    print('Calculating Minkowski dsitances...1-5 minutes')\n",
    "    %time Xy_scaled_minkowski = squareform(pdist(Xy_scaled, 'minkowski'))\n",
    "    np.save(os.path.join(DATA_PATH,'car-insurance-claim-data/car_insurance_claim_Xy_scaled_minkowski.npy'),Xy_scaled_minkowski)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the pyclustering implmentation in order to have comparison's to the similar, yet more memory efficient, PAM and CLARANS medoid methods.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Beware! pyclustering return clusters in an $n$ length list of lists, where $n=k$ and $list$[$n$][$i$] is the index postion from the input distance matrix. Here i use a dataframe to convert the pyclustering output to the form expected by scikit-learn silhouette score.\n",
    "    \n",
    "- [see this issue](https://github.com/annoviko/pyclustering/issues/593)  \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find k clusters\n",
    "results_kmedoids = dict()\n",
    "\n",
    "k_cand = [3,7,15,30,45,60]\n",
    "#k_cand.extend(list(np.arange(10,55,5)))\n",
    "\n",
    "for k in k_cand:\n",
    "    # initiate k random medoids - sets k clusters\n",
    "    initial_medoids = np.random.randint(0,1000,size=k)\n",
    "    kmedoids_instance = kmedoids(Xy_scaled_minkowski,initial_medoids, data_type='distance_matrix')    \n",
    "\n",
    "    # run cluster analysis and obtain results\n",
    "    %time kmedoids_instance.process()\n",
    "    clusters = kmedoids_instance.get_clusters()\n",
    "    medoids = kmedoids_instance.get_medoids()\n",
    "\n",
    "    # convert cluster output\n",
    "    cluster_array = pd.DataFrame([(x,e) for e,i in enumerate(clusters) for x in i if len(i)>1]).sort_values(by=0)[1].values\n",
    "    \n",
    "    # score\n",
    "    score1 = silhouette_score(Xy_scaled_minkowski, cluster_array, metric='precomputed')\n",
    "    score2 = silhouette_score(Xy_scaled, cluster_array,metric='correlation')\n",
    "    \n",
    "    # store\n",
    "    results_kmedoids[k] = {'k':cluster_array,'s1':score1,'s2':score2}\n",
    "    \n",
    "plt.plot([i for i in results_kmedoids.keys()],[i['s1'] for i in results_kmedoids.values()],label='Minkowski')\n",
    "plt.plot([i for i in results_kmedoids.keys()],[i['s2'] for i in results_kmedoids.values()],label='correlation')\n",
    "plt.legend()\n",
    "plt.xticks(k_cand);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 CLARANS (transformed and scaled features)\n",
    "### *Clustering Large Applications based on RANdomized Search*\n",
    " - [clarans in python](https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.clarans import clarans\n",
    "from pyclustering.utils import euclidean_distance_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_scaled_list = Xy_scaled.to_numpy().tolist()\n",
    "len(Xy_scaled_list)\n",
    "\n",
    "# sample euclid\n",
    "# [euclidean_distance_square(Xy_scaled_list[0],Xy_scaled_list[i]) for i in range(4)]\n",
    "\n",
    "\"\"\"\n",
    "data: Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "number_clusters: amount of clusters that should be allocated.\n",
    "numlocal: the number of local minima obtained (amount of iterations for solving the problem).\n",
    "maxneighbor: the maximum number of neighbors examined.     \n",
    "The higher the value of maxneighbor, the closer is CLARANS to K-Medoids, and the longer is each search of a local minima.\n",
    "\"\"\"\n",
    "\n",
    "clarans_instance = clarans(data=Xy_scaled_list[:50], number_clusters=3, numlocal=1, maxneighbor=2)\n",
    "%time clarans_instance.process()\n",
    "clusters = clarans_instance.get_clusters()\n",
    "\n",
    "#returns the clusters & medoids\n",
    "clusters = clarans_instance.get_clusters()\n",
    "medoids = clarans_instance.get_medoids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK! Something strange is happening here.**\n",
    "\n",
    "Our supposedly efficient CLARANS is grinding through on our 10302*49 dimension data, taking some 5 minutes to process just 1000 rows. I think that we are hitting the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality), as the CLARANS implementaiton in pyclustering uses a euclidean distance metric. Euclidean distance breaks down in high dimensional space (see [here](https://towardsdatascience.com/how-to-cluster-in-high-dimensions-4ef693bacc6) and [here](https://en.wikipedia.org/wiki/Clustering_high-dimensional_data). There are several things we could try here:\n",
    "\n",
    " - One option would be to re-write the CLARANS method using a distance metric that is robust to high dimensionality, such as the Manhattan or Minkowski distance.\n",
    " - Apply an alternative clustering approach that is less affected by high dimensionality such as [spectral](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering) or a Hierarchal approach as above or [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html).\n",
    " - Anoter would be to reduce dimensionality using [*tSNE*](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (see guides [here](https://www.datacamp.com/community/tutorials/introduction-t-sne) and [here](https://towardsdatascience.com/entity-embedding-using-t-sne-973cb5c730d7))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=3, verbose=1, random_state=0, n_iter=500)\n",
    "tsne = tsne_model.fit_transform(Xy_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find k clusters\n",
    "results_tsne_clarans = dict()\n",
    "\n",
    "k_cand = [3,7,15,30,45,60]\n",
    "#k_cand.extend(list(np.arange(10,55,5)))\n",
    "\n",
    "\"\"\"\n",
    "data: Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "number_clusters: amount of clusters that should be allocated.\n",
    "numlocal: the number of local minima obtained (amount of iterations for solving the problem).\n",
    "maxneighbor: the maximum number of neighbors examined.     \n",
    "The higher the value of maxneighbor, the closer is CLARANS to K-Medoids, and the longer is each search of a local minima.\n",
    "\"\"\"\n",
    "\n",
    "for k in k_cand:\n",
    "    clarans_instance = clarans(data=tsne.tolist(), number_clusters=k, numlocal=1, maxneighbor=2)\n",
    "    %time clarans_instance.process()\n",
    "    #returns the clusters & medoids\n",
    "    clusters = clarans_instance.get_clusters()\n",
    "    medoids = clarans_instance.get_medoids()\n",
    "\n",
    "    # convert cluster output\n",
    "    cluster_array = pd.DataFrame([(x,e) for e,i in enumerate(clusters) for x in i if len(i)>1]).sort_values(by=0)[1].values\n",
    "    \n",
    "    # score\n",
    "    score1 = 0 # silhouette_score(Xy_scaled_minkowski, cluster_array, metric='precomputed')\n",
    "    score2 = silhouette_score(Xy_scaled, cluster_array,metric='correlation')\n",
    "    \n",
    "    # store\n",
    "    results_tsne_clarans[k] = {'k':cluster_array,'s1':score1,'s2':score2}\n",
    "    \n",
    "plt.plot([i for i in results_tsne_clarans.keys()],[i['s1'] for i in results_tsne_clarans.values()],label='Minkowski')\n",
    "plt.plot([i for i in results_tsne_clarans.keys()],[i['s2'] for i in results_tsne_clarans.values()],label='correlation')\n",
    "plt.legend()\n",
    "plt.xticks(k_cand);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(20, 20),sharex=True,sharey=True)\n",
    "\n",
    "for j,ax in zip(results_tsne_clarans.keys(),axs.flatten()):\n",
    "    plotting = pd.DataFrame([(e,i) for e,i in enumerate(results_tsne_clarans[j]['k'])],columns=['id','k']).sort_values(by='id').sort_values(by='id')\n",
    "    plotting['x'] = tsne[:,0]\n",
    "    plotting['y'] = tsne[:,1]\n",
    "    groups = plotting.groupby('k')\n",
    "    \n",
    "    for name, group in groups:\n",
    "        ax.plot(group['x'], group['y'], marker='o', linestyle='', label=name)\n",
    "        #ax.legend()\n",
    "        ax.set_title(f'CLARANS-$tSNE$ ($k$={j})')\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 PAM partition-around-medoids (transformed and scaled features)\n",
    " - [PAM  is a variation of K-medoids](https://stats.stackexchange.com/a/141208/100439)\n",
    " - [Self defined PAM k-medoids in python](https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do list\n",
    "\n",
    "### Inspect value's between sets of clusters and intra-clusters:\n",
    " - Descriptive statistics\n",
    " - Categorical features: with chi squared\n",
    " - Numerical features: t-test, non-paramteric tests, correlaiton\n",
    " \n",
    "\n",
    "- [Notes on data mining](https://gist.github.com/AKST/d27b9006bb0f9670e370)\n",
    "- http://eric.univ-lyon2.fr/~ricco/cours/slides/en/classif_interpretation.pdf\n",
    "- https://online.stat.psu.edu/stat414/node/116/\n",
    "- https://www2.cs.duke.edu/courses/spring18/compsci216/lectures/07-clustering.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *References*\n",
    "\n",
    "\n",
    "- https://www.thinkdatascience.com/post/2019-12-16-introducing-python-package-gower/\n",
    "- https://pypi.org/project/gower/\n",
    "- https://medium.com/analytics-vidhya/gowers-distance-899f9c4bd553\n",
    "- https://www.researchgate.net/publication/324073652_Sparse_dynamical_Boltzmann_machine_for_reconstructing_complex_networks_with_binary_dynamics\n",
    "- https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3\n",
    "- https://medium.com/@rumman1988/clustering-categorical-and-numerical-datatype-using-gower-distance-ab89b3aa90d9\n",
    "- https://www2.cs.duke.edu/courses/spring18/compsci216/lectures/07-clustering.pdf\n",
    "- https://towardsdatascience.com/hierarchical-clustering-on-categorical-data-in-r-a27e578f2995\n",
    "- https://www.researchgate.net/post/What_is_the_best_way_for_cluster_analysis_when_you_have_mixed_type_of_data_categorical_and_scale\n",
    "- https://www.google.com/search?client=firefox-b-d&q=python+gower+distance\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html\n",
    "- https://discuss.analyticsvidhya.com/t/clustering-technique-for-mixed-numeric-and-categorical-variables/6753\n",
    "- https://stackoverflow.com/questions/24196897/r-distance-matrix-and-clustering-for-mixed-and-large-dataset\n",
    "- https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/\n",
    "- https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\n",
    "- https://rpubs.com/data_feelings/msda_data621_hw4\n",
    "- https://pypi.org/project/gower/\n",
    "- https://scikit-learn-extra.readthedocs.io/en/latest/generated/sklearn_extra.cluster.KMedoids.html\n",
    "- https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05\n",
    "- https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/pam\n",
    "- https://github.com/annoviko/pyclustering/issues/499\n",
    "- https://stats.stackexchange.com/questions/2717/clustering-with-a-distance-matrix\n",
    "- https://www.kaggle.com/fabiendaniel/customer-segmentation\n",
    "- https://dkopczyk.quantee.co.uk/claim-prediction/ (http://web.archive.org/web/20190429040211/https://dkopczyk.quantee.co.uk/claim-prediction/)\n",
    "- https://www.casact.org/pubs/dpp/dpp08/08dpp170.pdf\n",
    "- https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4\n",
    "- https://www.uio.no/studier/emner/matnat/math/nedlagte-emner/STK2510/v08/undervisningsmateriale/ch8b.pdf\n",
    "- https://github.com/annoviko/pyclustering/issues/499\n",
    "- https://stackoverflow.com/questions/3081066/what-techniques-exists-in-r-to-visualize-a-distance-matrix\n",
    "- https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n",
    "- https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\n",
    "- http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf\n",
    "- https://www.researchgate.net/post/What_is_the_best_way_for_cluster_analysis_when_you_have_mixed_type_of_data_categorical_and_scale\n",
    "- https://towardsdatascience.com/hierarchical-clustering-on-categorical-data-in-r-a27e578f2995\n",
    "- https://gist.github.com/AKST/d27b9006bb0f9670e370\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
