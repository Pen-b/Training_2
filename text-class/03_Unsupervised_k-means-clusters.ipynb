{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r'P:\\\\MyWork\\\\cass-property\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(path+r'data\\\\raw\\\\data_02.csv',low_memory=False, header=[0,1], encoding='latin-1') # read raw data (commas removed in excel - BI issue)\n",
    "dat = pd.DataFrame(dat.to_records(index=False)) # as records\n",
    "dat.columns = [re.sub(\"Unnamed: [0-9]|[0-9]_level_0\", '', x) for x in dat.columns]\n",
    "dat.columns = [re.sub(\"[(),']\", '', x).replace(' ', '_').lower() for x in dat.columns] # name cols\n",
    "len(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat['er_cause_of_loss'].value_counts().to_csv('codes.csv')\n",
    "#dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.dropna(subset=['er_exposure_reference'], inplace=True) # drop rows with no exposures\n",
    "dat.dropna(axis=1, how='all', inplace=True) # drop columns all null\n",
    "dat['uid'] = np.arange(len(dat)) # add numeric ID\n",
    "len(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(12,15))\n",
    "pd.DataFrame(dat.isnull().sum()/len(dat)).sort_values(by=0, ascending=False).plot.barh(ax=axs, legend=False, fontsize=15)\n",
    "plt.xlim(0,1);\n",
    "plt.xticks(np.arange(0,1.1,.1));\n",
    "axs.set_xticklabels(np.arange(0,110,10),fontsize=12);\n",
    "axs.set_xlabel('% missing',fontsize=15);\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+'reports/figures/missing.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat['er_loss_location_county'].value_counts()\n",
    "dat['er_cause_of_loss'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['er_cause_of_loss'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "#### target\n",
    " - er_cause_of_loss. BUT this has known issues as many are misaleblled. \n",
    "\n",
    "#### features\n",
    " - 'er_claim_lead_narrative'\n",
    " - 'er_claim_description'\n",
    " - 'er_loss_details'\n",
    " - 'er_exposure_description'\n",
    " - 'er_claim_narrative'\n",
    " \n",
    "#### additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['er_cause_of_loss']\n",
    "dat[target[0]].value_counts().plot.bar(figsize=(15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['er_claim_lead_narrative','er_claim_description',\n",
    "            'er_loss_details','er_exposure_description','er_claim_narrative']\n",
    "\n",
    "dat[features].loc[dat['er_cause_of_loss']=='Rain'].values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dat[target+features+['uid']].copy(deep=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge text data to one field\n",
    "data['doc'] = data[features].fillna('').apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['doc'] = data['doc'].replace('[^0-9a-zA-Z]+', ' ', regex=True).str.lower().str.strip()\n",
    "data['doc'] = data['doc'].replace('[^a-zA-Z]+', ' ', regex=True).str.lower().str.strip()\n",
    "data['doc'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['doc'] = data['doc'].replace('block for all losses', '', regex=True)\n",
    "data['doc'] = data['doc'].replace('block entry for all losses', '', regex=True)\n",
    "data['doc'] = data['doc'].replace('damage noc', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop short docs # experiment to set length\n",
    "data.drop(data.index[list(data.loc[data['doc'].str.len()<4].index)], inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove single characters\n",
    "data['doc'] = data['doc'].apply(lambda x: re.sub(r'\\b\\w\\b', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "# tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(np.concatenate([x.split() for x in data.doc])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list\n",
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations = pd.read_csv(r'P:\\MyWork\\ner_resources\\country-nationality.txt', encoding='utf-8', sep=',',header=0,\n",
    "                 usecols=[1,2,3,4],names=['c1','c2', 'nm', 'nt'])\n",
    "\n",
    "nat = []\n",
    "for col in nations.columns:\n",
    "    nat.append(list(nations[col].str.lower().unique()))\n",
    "nat = [item for sublist in nat for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities = list(pd.read_csv(r'P:\\MyWork\\ner_resources\\us-cities.txt', encoding='utf-8')['city'].str.lower())\n",
    "us_cities = [x for x in us_cities if isinstance(x, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to remove stop words\n",
    "filename = r'P:\\MyWork\\ner_resources\\beaz-stopwords'\n",
    "\n",
    "#Read\n",
    "with open(filename, 'r') as f:\n",
    "    stopwords = [line.rstrip(\"\\n\").replace(\"\\'\", \"\") for line in f]\n",
    "beaz_stopwords = list(set(stopwords))\n",
    "beaz_stopwords = [x for x in beaz_stopwords if isinstance(x, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us = pd.read_csv(r'P:\\MyWork\\ner_resources\\US\\US.txt', encoding='utf-8', sep='\\t', header=None,index_col=None,\n",
    "                 usecols=[0,3,4],names=['c','state', 'sc'])\n",
    "us_places = []\n",
    "for col in us.columns:\n",
    "    us_places.append(list(us[col].str.lower().unique()))\n",
    "us_places = [item for sublist in us_places for item in sublist]\n",
    "us_places = [x for x in us_places if isinstance(x, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brokers = ['risc', 'concannon', 'phelan', 'jensvold',\n",
    "           'charles', 'windsor', 'acton', 'hall', 'wright',\n",
    "           'crawford','collis', 'gab','collis', 'vericlaim','johnson', 'crump', 'wilcox', 'hiscox',\n",
    "           'tapco', 'cunningham', 'quebec','proctor', 'heath', 'thompson', 'aol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_words.union(us_places+us_cities+beaz_stopwords+brokers+nat)\n",
    "stops = [x for x in stop_words if isinstance(x, str)]\n",
    "stops = list(set(stops))\n",
    "stops = ['\\\\b'+re.sub(r'[^a-zA-Z ]+', '', x)+'\\\\b' for x in stops] # add breaks around word for whole word search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['doc_stop'] = data['doc'].str.split().apply(lambda x: [item for item in x if item not in stop_words]).str.join(' ') # only does single stop_words\n",
    "data['doc_stop'] = data['doc'].copy()\n",
    "data['doc_stop'].replace(to_replace=stops, value='', inplace=True, regex=True) # does multi-len stop words e.g. \"north carolina\"\n",
    "\n",
    "data['doc_stop'].replace('\\s+', ' ', regex=True, inplace=True) # remove multi whitespaces\n",
    "data['doc_stop'].replace(' ', np.nan, inplace=True) # replace empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autocorrect import spell\n",
    "# data['doc_stop_spell'] = data['doc_stop'].copy()\n",
    "# data['doc_stop_spell'] = \n",
    "# data['doc_stop_spell'].dropna().str.split().apply(lambda x: [spell(item) for item in x]).str.join(' ')\n",
    "# data[['doc','doc_stop', 'doc_stop_spell']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "data['doc_stop_stem'] = data['doc_stop'].copy()\n",
    "data['doc_stop_stem'] = data['doc_stop_stem'].dropna().str.split().apply(lambda x: [ps.stem(item) for item in x]).str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dat[['uid','sr_section_reference','er_exposure_reference']].merge(data, how='right', left_on='uid', right_on='uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster\n",
    "\n",
    "- https://stackoverflow.com/questions/27889873/clustering-text-documents-using-scikit-learn-kmeans-in-python\n",
    "- http://brandonrose.org/clustering\n",
    "- https://pythonprogramminglanguage.com/kmeans-text-clustering/\n",
    "- https://stackoverflow.com/questions/28160335/plot-a-document-tfidf-2d-graph/28205420#28205420\n",
    "- https://stackoverflow.com/questions/36946510/from-text-to-k-means-vectors-input\n",
    "- http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#\n",
    "- http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "- http://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "- http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'storm'\n",
    "print(data['doc_stop_stem'].loc[data['doc_stop_stem'].str.contains(term)==True].count() / len(data))\n",
    "#data['doc_stop_stem'].loc[data['doc_stop_stem'].str.contains(term)==True][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TV = TfidfVectorizer(stop_words=set(stops), ngram_range=(1,3),\n",
    "                     min_df=0.0, max_df=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc idf sparse matrix \n",
    "X = TV.fit_transform(data['doc_stop_stem'].dropna())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_score = {}\n",
    "for k in np.arange(20,120,10):\n",
    "    KM = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=1, n_jobs=-2)\n",
    "    k_score[k] = KM.fit(X).score(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k,v in k_score.items():\n",
    "    #print(k,v)\n",
    "plt.plot(k_score.keys(),k_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=25, init='k-means++', max_iter=300, n_init=1, n_jobs=-2)\n",
    "KM.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhoutte scoring\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# model.labels_.tolist() # equivalnet to predcit(X)\n",
    "silhouette_score(X, KM.labels_, sample_size=10000, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_centroids = KM.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = TV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters={}\n",
    "for i in range(25):\n",
    "    ter = []\n",
    "    print( \"\\nCluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        #print(i,ind)\n",
    "        print( '%d %s' %( ind,terms[ind]), end=',')\n",
    "        ter.append(terms[ind])\n",
    "    clusters[i] = ter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cluster']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['doc_stop_stem'].notnull(),'cluster'] = KM.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA / SVD Dimension reduction \n",
    "\n",
    " - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD.fit_transform\n",
    "\n",
    " - https://stackoverflow.com/questions/42882207/plot-k-means-clusters-after-truncatedsvd-python\n",
    "\n",
    "#### n_components\n",
    " - https://chrisalbon.com/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/\n",
    " - https://stackoverflow.com/questions/48424084/number-of-components-trucated-svd\n",
    " - https://cstheory.stackexchange.com/questions/21487/when-to-use-the-johnson-lindenstrauss-lemma-over-svd/21489#21489\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = TruncatedSVD(n_components=2)\n",
    "data2D = SVD.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data2D[:,0], data2D[:,1],cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nltk]",
   "language": "python",
   "name": "conda-env-nltk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
